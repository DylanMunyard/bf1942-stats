---
# PVC for backup storage (temporary holding before transfer to Proxmox)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: bf42-stats
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 10Gi

---
# PVC for Proxmox mount (this should be configured to mount your Proxmox backup location)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: proxmox-backup-pvc
  namespace: bf42-stats
spec:
  accessModes:
    - ReadWriteOnce
  # You'll need to configure this to use a storage class that maps to your Proxmox backup mount
  # This might be NFS, hostPath, or another storage class depending on your setup
  storageClassName: local-path  # Change this to match your Proxmox backup storage class
  resources:
    requests:
      storage: 50Gi

---
# ConfigMap containing backup scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-scripts
  namespace: bf42-stats
data:
  backup-sqlite.sh: |
    #!/bin/bash

    # SQLite Backup Script for BF1942 Stats
    # This script creates compressed backups of the SQLite database

    set -euo pipefail

    # Configuration
    DB_PATH="${DB_PATH:-/mnt/data/playertracker.db}"
    BACKUP_DIR="${BACKUP_DIR:-/mnt/backup}"
    RETENTION_DAYS="${RETENTION_DAYS:-30}"
    TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
    BACKUP_NAME="playertracker_${TIMESTAMP}.db"
    COMPRESSED_BACKUP="${BACKUP_NAME}.gz"

    # Logging function
    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    }

    # Error handling
    error_exit() {
        log "ERROR: $1"
        exit 1
    }

    # Check if source database exists
    if [[ ! -f "$DB_PATH" ]]; then
        error_exit "Source database not found: $DB_PATH"
    fi

    # Create backup directory if it doesn't exist
    mkdir -p "$BACKUP_DIR" || error_exit "Failed to create backup directory: $BACKUP_DIR"

    # Check available disk space (require at least 100MB free)
    AVAILABLE_SPACE=$(df "$BACKUP_DIR" | awk 'NR==2 {print $4}')
    if [[ $AVAILABLE_SPACE -lt 102400 ]]; then
        error_exit "Insufficient disk space. Available: ${AVAILABLE_SPACE}KB, Required: 100MB"
    fi

    log "Starting SQLite backup process..."
    log "Source: $DB_PATH"
    log "Destination: $BACKUP_DIR/$COMPRESSED_BACKUP"

    # Perform SQLite backup using .backup command for consistency
    log "Creating SQLite backup..."
    sqlite3 "$DB_PATH" ".backup $BACKUP_DIR/$BACKUP_NAME" || error_exit "SQLite backup failed"

    # Verify backup file was created
    if [[ ! -f "$BACKUP_DIR/$BACKUP_NAME" ]]; then
        error_exit "Backup file was not created: $BACKUP_DIR/$BACKUP_NAME"
    fi

    # Get original file size for logging
    ORIGINAL_SIZE=$(stat -c%s "$DB_PATH")
    BACKUP_SIZE=$(stat -c%s "$BACKUP_DIR/$BACKUP_NAME")

    log "Original database size: $(numfmt --to=iec $ORIGINAL_SIZE)"
    log "Backup file size: $(numfmt --to=iec $BACKUP_SIZE)"

    # Compress the backup
    log "Compressing backup..."
    gzip "$BACKUP_DIR/$BACKUP_NAME" || error_exit "Compression failed"

    # Verify compressed file was created
    if [[ ! -f "$BACKUP_DIR/$COMPRESSED_BACKUP" ]]; then
        error_exit "Compressed backup file was not created: $BACKUP_DIR/$COMPRESSED_BACKUP"
    fi

    COMPRESSED_SIZE=$(stat -c%s "$BACKUP_DIR/$COMPRESSED_BACKUP")
    COMPRESSION_RATIO=$(echo "scale=1; $COMPRESSED_SIZE * 100 / $BACKUP_SIZE" | bc)

    log "Compressed backup size: $(numfmt --to=iec $COMPRESSED_SIZE) (${COMPRESSION_RATIO}% of original)"

    # Clean up old backups
    log "Cleaning up backups older than $RETENTION_DAYS days..."
    find "$BACKUP_DIR" -name "playertracker_*.db.gz" -type f -mtime +$RETENTION_DAYS -exec rm {} \; || log "WARNING: Failed to clean up some old backups"

    # Count remaining backups
    BACKUP_COUNT=$(find "$BACKUP_DIR" -name "playertracker_*.db.gz" -type f | wc -l)
    log "Backup completed successfully. Total backups: $BACKUP_COUNT"

    # Set appropriate file permissions
    chmod 640 "$BACKUP_DIR/$COMPRESSED_BACKUP"

    # Output backup information for Kubernetes logs
    cat << EOF
    BACKUP_STATUS=SUCCESS
    BACKUP_FILE=$BACKUP_DIR/$COMPRESSED_BACKUP
    ORIGINAL_SIZE=$ORIGINAL_SIZE
    COMPRESSED_SIZE=$COMPRESSED_SIZE
    COMPRESSION_RATIO=${COMPRESSION_RATIO}%
    BACKUP_COUNT=$BACKUP_COUNT
    EOF

    log "SQLite backup process completed successfully!"

  backup-clickhouse.sh: |
    #!/bin/bash

    # ClickHouse Backup Script for BF1942 Stats
    # This script creates compressed backups of ClickHouse database tables

    set -euo pipefail

    # Configuration
    CLICKHOUSE_URL="${CLICKHOUSE_URL:-http://clickhouse-service.clickhouse:8123}"
    CLICKHOUSE_USER="${CLICKHOUSE_USER:-default}"
    CLICKHOUSE_PASSWORD="${CLICKHOUSE_PASSWORD:-}"
    CLICKHOUSE_DATABASE="${CLICKHOUSE_DATABASE:-default}"
    BACKUP_DIR="${BACKUP_DIR:-/mnt/backup}"
    RETENTION_DAYS="${RETENTION_DAYS:-30}"
    TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
    BACKUP_PREFIX="clickhouse_${TIMESTAMP}"

    # Tables to backup (based on your application)
    TABLES=(
        "player_metrics"
        "player_rounds"
        "player_achievements"
        "gamification_achievements"
        "team_killer_metrics"
        "server_statistics"
    )

    # Logging function
    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    }

    # Error handling
    error_exit() {
        log "ERROR: $1"
        exit 1
    }

    # ClickHouse query function
    clickhouse_query() {
        local query="$1"
        local auth_header=""
        
        if [[ -n "$CLICKHOUSE_PASSWORD" ]]; then
            auth_header="--header X-ClickHouse-Key:$CLICKHOUSE_PASSWORD"
        fi
        
        curl -s --fail \
            --header "X-ClickHouse-User:$CLICKHOUSE_USER" \
            $auth_header \
            --data "$query" \
            "$CLICKHOUSE_URL" || return 1
    }

    # Test ClickHouse connectivity
    test_clickhouse_connection() {
        log "Testing ClickHouse connectivity..."
        local result
        result=$(clickhouse_query "SELECT 1") || error_exit "Cannot connect to ClickHouse at $CLICKHOUSE_URL"
        
        if [[ "$result" != "1" ]]; then
            error_exit "ClickHouse connection test failed. Response: $result"
        fi
        
        log "ClickHouse connectivity test passed"
    }

    # Get table list from ClickHouse
    get_existing_tables() {
        local query="SELECT name FROM system.tables WHERE database = '$CLICKHOUSE_DATABASE' AND engine NOT LIKE '%View%' FORMAT TSV"
        clickhouse_query "$query" | grep -E "^($(IFS='|'; echo "${TABLES[*]}"))\$" || echo ""
    }

    # Backup single table
    backup_table() {
        local table="$1"
        local backup_file="$BACKUP_DIR/${BACKUP_PREFIX}_${table}.csv.gz"
        
        log "Backing up table: $table"
        
        # Check if table exists and get row count
        local row_count
        row_count=$(clickhouse_query "SELECT COUNT(*) FROM ${CLICKHOUSE_DATABASE}.${table}") || {
            log "WARNING: Cannot access table $table, skipping..."
            return 0
        }
        
        if [[ "$row_count" == "0" ]]; then
            log "Table $table is empty, creating empty backup file"
            touch "$backup_file"
            return 0
        fi
        
        log "Table $table contains $row_count rows"
        
        # Create the backup query
        local query="SELECT * FROM ${CLICKHOUSE_DATABASE}.${table} FORMAT CSV"
        
        # Execute backup with compression
        if clickhouse_query "$query" | gzip > "$backup_file"; then
            local file_size
            file_size=$(stat -c%s "$backup_file")
            log "Table $table backup completed: $(numfmt --to=iec $file_size)"
        else
            error_exit "Failed to backup table: $table"
        fi
        
        # Verify backup file was created and is not empty (unless table was empty)
        if [[ ! -f "$backup_file" ]] || ([[ "$row_count" != "0" ]] && [[ ! -s "$backup_file" ]]); then
            error_exit "Backup file verification failed for table: $table"
        fi
        
        # Set appropriate permissions
        chmod 640 "$backup_file"
    }

    # Main backup process
    main() {
        log "Starting ClickHouse backup process..."
        log "ClickHouse URL: $CLICKHOUSE_URL"
        log "Database: $CLICKHOUSE_DATABASE"
        log "Backup directory: $BACKUP_DIR"
        
        # Create backup directory if it doesn't exist
        mkdir -p "$BACKUP_DIR" || error_exit "Failed to create backup directory: $BACKUP_DIR"
        
        # Check available disk space (require at least 500MB free for ClickHouse backups)
        local available_space
        available_space=$(df "$BACKUP_DIR" | awk 'NR==2 {print $4}')
        if [[ $available_space -lt 512000 ]]; then
            error_exit "Insufficient disk space. Available: ${available_space}KB, Required: 500MB"
        fi
        
        # Test connection
        test_clickhouse_connection
        
        # Get list of existing tables
        local existing_tables
        existing_tables=$(get_existing_tables)
        
        if [[ -z "$existing_tables" ]]; then
            log "WARNING: No target tables found in ClickHouse database"
            # Create empty marker file to indicate backup ran but found no tables
            touch "$BACKUP_DIR/${BACKUP_PREFIX}_empty.marker"
        else
            log "Found tables to backup: $existing_tables"
            
            # Backup each existing table
            local success_count=0
            local total_size=0
            
            while IFS= read -r table; do
                if [[ -n "$table" ]]; then
                    backup_table "$table"
                    ((success_count++))
                    
                    # Add to total size
                    local table_backup_file="$BACKUP_DIR/${BACKUP_PREFIX}_${table}.csv.gz"
                    if [[ -f "$table_backup_file" ]]; then
                        local file_size
                        file_size=$(stat -c%s "$table_backup_file")
                        ((total_size += file_size))
                    fi
                fi
            done <<< "$existing_tables"
            
            log "Successfully backed up $success_count tables"
            log "Total backup size: $(numfmt --to=iec $total_size)"
        fi
        
        # Clean up old backups
        log "Cleaning up backups older than $RETENTION_DAYS days..."
        find "$BACKUP_DIR" -name "clickhouse_*.csv.gz" -type f -mtime +$RETENTION_DAYS -exec rm {} \; || log "WARNING: Failed to clean up some old backups"
        find "$BACKUP_DIR" -name "clickhouse_*.marker" -type f -mtime +$RETENTION_DAYS -exec rm {} \; || log "WARNING: Failed to clean up some old marker files"
        
        # Count remaining backups
        local backup_count
        backup_count=$(find "$BACKUP_DIR" -name "clickhouse_*.csv.gz" -o -name "clickhouse_*.marker" | wc -l)
        log "Backup cleanup completed. Total backup files: $backup_count"
        
        # Create backup manifest
        local manifest_file="$BACKUP_DIR/${BACKUP_PREFIX}_manifest.txt"
        {
            echo "ClickHouse Backup Manifest"
            echo "Created: $(date)"
            echo "Database: $CLICKHOUSE_DATABASE"
            echo "Tables backed up:"
            if [[ -n "$existing_tables" ]]; then
                while IFS= read -r table; do
                    if [[ -n "$table" ]]; then
                        local table_backup_file="$BACKUP_DIR/${BACKUP_PREFIX}_${table}.csv.gz"
                        if [[ -f "$table_backup_file" ]]; then
                            local file_size row_count_result
                            file_size=$(stat -c%s "$table_backup_file")
                            row_count_result=$(clickhouse_query "SELECT COUNT(*) FROM ${CLICKHOUSE_DATABASE}.${table}" 2>/dev/null || echo "unknown")
                            echo "  - $table: $row_count_result rows, $(numfmt --to=iec $file_size)"
                        fi
                    fi
                done <<< "$existing_tables"
            else
                echo "  - No tables found"
            fi
        } > "$manifest_file"
        
        chmod 640 "$manifest_file"
        
        # Output backup information for Kubernetes logs
        cat << EOF
    BACKUP_STATUS=SUCCESS
    BACKUP_PREFIX=$BACKUP_PREFIX
    BACKUP_DIR=$BACKUP_DIR
    TABLES_BACKED_UP=$success_count
    TOTAL_SIZE=${total_size:-0}
    BACKUP_COUNT=$backup_count
    EOF
        
        log "ClickHouse backup process completed successfully!"
    }

    # Run main function
    main "$@"

  backup-transfer.sh: |
    #!/bin/bash

    # Backup Transfer Script for BF1942 Stats
    # This script transfers backups from Kubernetes storage to Proxmox host mount
    # Designed to run as a separate job after backup creation

    set -euo pipefail

    # Configuration
    BACKUP_SOURCE_DIR="${BACKUP_SOURCE_DIR:-/mnt/backup}"
    BACKUP_DEST_DIR="${BACKUP_DEST_DIR:-/mnt/proxmox-backup}"
    RETENTION_DAYS="${RETENTION_DAYS:-30}"
    SYNC_MODE="${SYNC_MODE:-copy}" # copy or move

    # Logging function
    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    }

    # Error handling
    error_exit() {
        log "ERROR: $1"
        exit 1
    }

    # Check if directories exist
    check_directories() {
        if [[ ! -d "$BACKUP_SOURCE_DIR" ]]; then
            error_exit "Source backup directory not found: $BACKUP_SOURCE_DIR"
        fi
        
        # Create destination directory if it doesn't exist
        mkdir -p "$BACKUP_DEST_DIR" || error_exit "Failed to create destination directory: $BACKUP_DEST_DIR"
        
        # Test write permissions on destination
        local test_file="$BACKUP_DEST_DIR/.write_test_$$"
        if ! touch "$test_file" 2>/dev/null; then
            error_exit "No write permissions on destination directory: $BACKUP_DEST_DIR"
        fi
        rm -f "$test_file"
        
        log "Directory checks passed"
        log "Source: $BACKUP_SOURCE_DIR"
        log "Destination: $BACKUP_DEST_DIR"
    }

    # Get disk usage information
    check_disk_space() {
        local source_usage dest_usage source_available dest_available
        
        source_usage=$(du -sh "$BACKUP_SOURCE_DIR" | cut -f1)
        source_available=$(df -h "$BACKUP_SOURCE_DIR" | awk 'NR==2 {print $4}')
        
        dest_usage=$(du -sh "$BACKUP_DEST_DIR" | cut -f1)
        dest_available=$(df -h "$BACKUP_DEST_DIR" | awk 'NR==2 {print $4}')
        
        log "Source directory usage: $source_usage (Available: $source_available)"
        log "Destination directory usage: $dest_usage (Available: $dest_available)"
        
        # Check if destination has enough space (rough estimate)
        local dest_available_kb dest_required_kb
        dest_available_kb=$(df "$BACKUP_DEST_DIR" | awk 'NR==2 {print $4}')
        dest_required_kb=$(du -s "$BACKUP_SOURCE_DIR" | cut -f1)
        
        if [[ $dest_available_kb -lt $dest_required_kb ]]; then
            log "WARNING: Destination may not have enough space"
            log "Available: ${dest_available_kb}KB, Required: ${dest_required_kb}KB"
        fi
    }

    # Transfer files
    transfer_backups() {
        local transferred_count=0
        local failed_count=0
        local total_size=0
        
        log "Starting backup transfer process..."
        log "Transfer mode: $SYNC_MODE"
        
        # Find all backup files from today and yesterday (in case job runs around midnight)
        local current_date previous_date
        current_date=$(date +"%Y%m%d")
        previous_date=$(date -d "1 day ago" +"%Y%m%d")
        
        # Transfer SQLite backups
        log "Transferring SQLite backups..."
        for file in "$BACKUP_SOURCE_DIR"/playertracker_${current_date}_*.db.gz "$BACKUP_SOURCE_DIR"/playertracker_${previous_date}_*.db.gz; do
            if [[ -f "$file" ]]; then
                local basename filename dest_file file_size
                basename=$(basename "$file")
                dest_file="$BACKUP_DEST_DIR/$basename"
                
                if [[ ! -f "$dest_file" ]]; then
                    log "Transferring: $basename"
                    
                    if [[ "$SYNC_MODE" == "move" ]]; then
                        if mv "$file" "$dest_file"; then
                            ((transferred_count++))
                            file_size=$(stat -c%s "$dest_file")
                            ((total_size += file_size))
                            log "Moved: $basename ($(numfmt --to=iec $file_size))"
                        else
                            log "ERROR: Failed to move $basename"
                            ((failed_count++))
                        fi
                    else
                        if cp "$file" "$dest_file"; then
                            ((transferred_count++))
                            file_size=$(stat -c%s "$dest_file")
                            ((total_size += file_size))
                            log "Copied: $basename ($(numfmt --to=iec $file_size))"
                        else
                            log "ERROR: Failed to copy $basename"
                            ((failed_count++))
                        fi
                    fi
                    
                    # Set appropriate permissions
                    chmod 640 "$dest_file" 2>/dev/null || log "WARNING: Could not set permissions for $dest_file"
                else
                    log "Skipping $basename (already exists at destination)"
                fi
            fi
        done
        
        # Transfer ClickHouse backups and manifests
        log "Transferring ClickHouse backups..."
        for file in "$BACKUP_SOURCE_DIR"/clickhouse_${current_date}_*.csv.gz "$BACKUP_SOURCE_DIR"/clickhouse_${current_date}_*.txt "$BACKUP_SOURCE_DIR"/clickhouse_${current_date}_*.marker \
                    "$BACKUP_SOURCE_DIR"/clickhouse_${previous_date}_*.csv.gz "$BACKUP_SOURCE_DIR"/clickhouse_${previous_date}_*.txt "$BACKUP_SOURCE_DIR"/clickhouse_${previous_date}_*.marker; do
            if [[ -f "$file" ]]; then
                local basename dest_file file_size
                basename=$(basename "$file")
                dest_file="$BACKUP_DEST_DIR/$basename"
                
                if [[ ! -f "$dest_file" ]]; then
                    log "Transferring: $basename"
                    
                    if [[ "$SYNC_MODE" == "move" ]]; then
                        if mv "$file" "$dest_file"; then
                            ((transferred_count++))
                            file_size=$(stat -c%s "$dest_file")
                            ((total_size += file_size))
                            log "Moved: $basename ($(numfmt --to=iec $file_size))"
                        else
                            log "ERROR: Failed to move $basename"
                            ((failed_count++))
                        fi
                    else
                        if cp "$file" "$dest_file"; then
                            ((transferred_count++))
                            file_size=$(stat -c%s "$dest_file")
                            ((total_size += file_size))
                            log "Copied: $basename ($(numfmt --to=iec $file_size))"
                        else
                            log "ERROR: Failed to copy $basename"
                            ((failed_count++))
                        fi
                    fi
                    
                    # Set appropriate permissions
                    chmod 640 "$dest_file" 2>/dev/null || log "WARNING: Could not set permissions for $dest_file"
                else
                    log "Skipping $basename (already exists at destination)"
                fi
            fi
        done
        
        log "Transfer completed: $transferred_count files transferred, $failed_count failed"
        log "Total transferred size: $(numfmt --to=iec $total_size)"
        
        if [[ $failed_count -gt 0 ]]; then
            error_exit "$failed_count file transfers failed"
        fi
    }

    # Clean up old backups from destination
    cleanup_old_backups() {
        log "Cleaning up old backups from destination (older than $RETENTION_DAYS days)..."
        
        local cleaned_count=0
        
        # Clean up SQLite backups
        while IFS= read -r -d '' file; do
            rm "$file" && ((cleaned_count++)) || log "WARNING: Failed to remove $file"
        done < <(find "$BACKUP_DEST_DIR" -name "playertracker_*.db.gz" -type f -mtime +$RETENTION_DAYS -print0 2>/dev/null || true)
        
        # Clean up ClickHouse backups
        while IFS= read -r -d '' file; do
            rm "$file" && ((cleaned_count++)) || log "WARNING: Failed to remove $file"
        done < <(find "$BACKUP_DEST_DIR" -name "clickhouse_*.csv.gz" -o -name "clickhouse_*.txt" -o -name "clickhouse_*.marker" -type f -mtime +$RETENTION_DAYS -print0 2>/dev/null || true)
        
        log "Cleaned up $cleaned_count old backup files"
    }

    # Create transfer report
    create_transfer_report() {
        local report_file="$BACKUP_DEST_DIR/transfer_report_$(date +"%Y%m%d_%H%M%S").txt"
        local total_backups
        total_backups=$(find "$BACKUP_DEST_DIR" -name "playertracker_*.db.gz" -o -name "clickhouse_*.csv.gz" -o -name "clickhouse_*.marker" | wc -l)
        
        {
            echo "Backup Transfer Report"
            echo "Generated: $(date)"
            echo "Source: $BACKUP_SOURCE_DIR"
            echo "Destination: $BACKUP_DEST_DIR"
            echo "Transfer mode: $SYNC_MODE"
            echo "Retention period: $RETENTION_DAYS days"
            echo ""
            echo "Current backups in destination:"
            find "$BACKUP_DEST_DIR" -name "playertracker_*.db.gz" -o -name "clickhouse_*.csv.gz" -o -name "clickhouse_*.marker" -o -name "clickhouse_*.txt" | sort | while read -r file; do
                if [[ -f "$file" ]]; then
                    local size mtime basename
                    basename=$(basename "$file")
                    size=$(stat -c%s "$file")
                    mtime=$(stat -c%Y "$file")
                    echo "  $basename - $(numfmt --to=iec $size) - $(date -d @$mtime)"
                fi
            done
        } > "$report_file"
        
        chmod 640 "$report_file"
        log "Transfer report created: $report_file"
        log "Total backups in destination: $total_backups"
    }

    # Main function
    main() {
        log "Starting backup transfer process..."
        
        # Validate configuration
        check_directories
        check_disk_space
        
        # Perform transfer
        transfer_backups
        
        # Clean up old files
        cleanup_old_backups
        
        # Create report
        create_transfer_report
        
        # Output status for Kubernetes logs
        cat << EOF
    TRANSFER_STATUS=SUCCESS
    SOURCE_DIR=$BACKUP_SOURCE_DIR
    DEST_DIR=$BACKUP_DEST_DIR
    SYNC_MODE=$SYNC_MODE
    RETENTION_DAYS=$RETENTION_DAYS
    EOF
        
        log "Backup transfer process completed successfully!"
    }

    # Run main function
    main "$@"

---
# CronJob for SQLite backup (twice daily at 2am and 2pm)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-sqlite-job
  namespace: bf42-stats
spec:
  # Run at 2:00 AM and 2:00 PM every day
  schedule: "0 2,14 * * *"
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 5
  successfulJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: backup-sqlite
        spec:
          restartPolicy: Never
          nodeSelector:
            kubernetes.io/hostname: bethany  # Same node as main app
          containers:
          - name: backup-sqlite
            image: alpine:3.18
            command: ["/bin/sh"]
            args: ["-c", "apk add --no-cache sqlite bc coreutils && /scripts/backup-sqlite.sh"]
            env:
            - name: DB_PATH
              value: "/mnt/data/playertracker.db"
            - name: BACKUP_DIR
              value: "/mnt/backup"
            - name: RETENTION_DAYS
              value: "30"
            volumeMounts:
            - name: data-volume
              mountPath: /mnt/data
              readOnly: true
            - name: backup-volume
              mountPath: /mnt/backup
            - name: scripts
              mountPath: /scripts
            resources:
              requests:
                memory: "64Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "500m"
          volumes:
          - name: data-volume
            persistentVolumeClaim:
              claimName: bf42-stats-pvc
          - name: backup-volume
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          - name: scripts
            configMap:
              name: backup-scripts
              defaultMode: 0755

---
# CronJob for ClickHouse backup (twice daily at 2:05 AM and 2:05 PM)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-clickhouse-job
  namespace: bf42-stats
spec:
  # Run at 2:05 AM and 2:05 PM every day (5 minutes after SQLite backup)
  schedule: "5 2,14 * * *"
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 5
  successfulJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: backup-clickhouse
        spec:
          restartPolicy: Never
          containers:
          - name: backup-clickhouse
            image: alpine:3.18
            command: ["/bin/sh"]
            args: ["-c", "apk add --no-cache curl gzip bc coreutils && /scripts/backup-clickhouse.sh"]
            env:
            - name: CLICKHOUSE_URL
              value: "http://clickhouse-service.clickhouse:8123"
            - name: CLICKHOUSE_USER
              value: "default"
            - name: CLICKHOUSE_DATABASE
              value: "default"
            - name: BACKUP_DIR
              value: "/mnt/backup"
            - name: RETENTION_DAYS
              value: "30"
            volumeMounts:
            - name: backup-volume
              mountPath: /mnt/backup
            - name: scripts
              mountPath: /scripts
            resources:
              requests:
                memory: "128Mi"
                cpu: "200m"
              limits:
                memory: "512Mi"
                cpu: "1000m"
          volumes:
          - name: backup-volume
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          - name: scripts
            configMap:
              name: backup-scripts
              defaultMode: 0755

---
# CronJob for transferring backups to Proxmox (twice daily at 2:15 AM and 2:15 PM)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-transfer-job
  namespace: bf42-stats
spec:
  # Run at 2:15 AM and 2:15 PM every day (10 minutes after backups complete)
  schedule: "15 2,14 * * *"
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 5
  successfulJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: backup-transfer
        spec:
          restartPolicy: Never
          nodeSelector:
            kubernetes.io/hostname: bethany  # Same node as main app
          containers:
          - name: backup-transfer
            image: alpine:3.18
            command: ["/bin/sh"]
            args: ["-c", "apk add --no-cache coreutils && /scripts/backup-transfer.sh"]
            env:
            - name: BACKUP_SOURCE_DIR
              value: "/mnt/backup"
            - name: BACKUP_DEST_DIR
              value: "/mnt/proxmox-backup"
            - name: RETENTION_DAYS
              value: "30"
            - name: SYNC_MODE
              value: "copy"  # or "move" if you want to move instead of copy
            volumeMounts:
            - name: backup-volume
              mountPath: /mnt/backup
            - name: proxmox-backup-volume
              mountPath: /mnt/proxmox-backup
            - name: scripts
              mountPath: /scripts
            resources:
              requests:
                memory: "64Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "500m"
          volumes:
          - name: backup-volume
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          - name: proxmox-backup-volume
            persistentVolumeClaim:
              claimName: proxmox-backup-pvc
          - name: scripts
            configMap:
              name: backup-scripts
              defaultMode: 0755